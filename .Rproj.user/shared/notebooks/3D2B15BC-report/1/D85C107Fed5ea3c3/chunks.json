{"chunk_definitions":[{"row":43,"row_count":1,"visible":true,"expansion_state":0,"options":{"engine":"r","label":"setup","include":false,"dev":"png"},"document_id":"219634DD","chunk_id":"csetup_chunk","chunk_label":"setup"},{"row":49,"row_count":1,"visible":true,"expansion_state":0,"options":{"engine":"r","code":["#' Boshu Jiang, s2196886","#' Add your own function definitions on this file.","","#' Log-Exponential density","#'","#' Compute the density or log-density for a Log-Exponential (LogExp)","#' distribution","#'","#' @param x vector of quantiles","#' @param rate vector of rates","#' @param log logical; if TRUE, the log-density is returned","","dlogexp <- function(x, rate = 1, log = FALSE) {","  result <- log(rate) + x - rate * exp(x)","  if (!log) {","    exp(result)","  }","  result","}","","#' Log-Sum-Exp","#'","#' Convenience function for computing log(sum(exp(x))) in a","#' numerically stable manner","#'","#' @param x numerical vector","","log_sum_exp <- function(x) {","  max_x <- max(x, na.rm = TRUE)","  max_x + log(sum(exp(x - max_x)))","}","","","#' wquantile ","#'","#' Calculates empirical sample quantiles with optional weights, for given probabilities. ","#' Like in quantile(), the smallest observation corresponds to a probability of 0 and the largest to a probability of 1. ","#' Interpolation between discrete values is done when type=7, as in quantile(). ","#' Use type=1 to only generate quantile values from the raw input samples.","#'","#' @param x numeric vector whose sample quantiles are wanted","#' NA and NaN values are not allowed in numeric vectors unless na.rm is TRUE","#' @param probs numeric vector of probabilities with values in [0,1]","#' @param na.rm logical; if true, any NA and NaN's are removed from x before the quantiles are computed","#' @param type numeric, 1 for no interpolation, or 7, for interpolated quantiles. Default is 7","#' @param weights\t numeric vector of non-negative weights, the same length as x, or NULL. The weights are normalised to sum to 1. If NULL, then wquantile(x) behaves the same as quantile(x), with equal weight for each sample value","","wquantile <- function (x, probs = seq(0, 1, 0.25), na.rm = FALSE, type = 7, ","                       weights = NULL, ...) ","{","  if (is.null(weights) || (length(weights) == 1)) {","    weights <- rep(1, length(x))","  }","  stopifnot(all(weights >= 0))","  stopifnot(length(weights) == length(x))","  if (length(x) == 1) {","    return(rep(x, length(probs)))","  }","  n <- length(x)","  q <- numeric(length(probs))","  reorder <- order(x)","  weights <- weights[reorder]","  x <- x[reorder]","  wecdf <- pmin(1, cumsum(weights)/sum(weights))","  if (type == 1) {","  }","  else {","    weights2 <- (weights[-n] + weights[-1])/2","    wecdf2 <- pmin(1, cumsum(weights2)/sum(weights2))","  }","  for (pr_idx in seq_along(probs)) {","    pr <- probs[pr_idx]","    if (pr <= 0) {","      q[pr_idx] <- x[1]","    }","    else if (pr >= 1) {","      q[pr_idx] <- x[n]","    }","    else {","      if (type == 1) {","        j <- 1 + pmax(0, pmin(n - 1, sum(wecdf <= pr)))","        q[pr_idx] <- x[j]","      }","      else {","        j <- 1 + pmax(0, pmin(n - 2, sum(wecdf2 <= pr)))","        g <- (pr - c(0, wecdf2)[j])/(wecdf2[j] - c(0, ","                                                   wecdf2)[j])","        q[pr_idx] <- (1 - g) * x[j] + g * x[j + 1]","      }","    }","  }","  q","}","","#' Compute empirical weighted cumulative distribution","#'","#' Version of `ggplot2::stat_ecdf` that adds a `weights` property for each","#' observation, to produce an empirical weighted cumulative distribution function.","#' The empirical cumulative distribution function (ECDF) provides an alternative","#' visualisation of distribution. Compared to other visualisations that rely on","#' density (like [geom_histogram()]), the ECDF doesn't require any","#' tuning parameters and handles both continuous and discrete variables.","#' The downside is that it requires more training to accurately interpret,","#' and the underlying visual tasks are somewhat more challenging.","#'","# @inheritParams layer","# @inheritParams geom_point","#' @param na.rm If `FALSE` (the default), removes missing values with","#'    a warning.  If `TRUE` silently removes missing values.","#' @param n if NULL, do not interpolate. If not NULL, this is the number","#'   of points to interpolate with.","#' @param pad If `TRUE`, pad the ecdf with additional points (-Inf, 0)","#'   and (Inf, 1)","#' @section Computed variables:","#' \\describe{","#'   \\item{x}{x in data}","#'   \\item{y}{cumulative density corresponding x}","#' }","#' @seealso wquantile","#' @export","#' @examples","#' library(ggplot2)","#'","#' n <- 100","#' df <- data.frame(","#'   x = c(rnorm(n, 0, 10), rnorm(n, 0, 10)),","#'   g = gl(2, n),","#'   w = c(rep(1/n, n), sort(runif(n))^sqrt(n))","#' )","#' ggplot(df, aes(x, weights = w)) + stat_ewcdf(geom = \"step\")","#'","#' # Don't go to positive/negative infinity","#' ggplot(df, aes(x, weights = w)) + stat_ewcdf(geom = \"step\", pad = FALSE)","#'","#' # Multiple ECDFs","#' ggplot(df, aes(x, colour = g, weights = w)) + stat_ewcdf()","#' ggplot(df, aes(x, colour = g, weights = w)) +","#'   stat_ewcdf() +","#'   facet_wrap(vars(g), ncol = 1)","","stat_ewcdf <- function(mapping = NULL, data = NULL,","                       geom = \"step\", position = \"identity\",","                       ...,","                       n = NULL,","                       pad = TRUE,","                       na.rm = FALSE,","                       show.legend = NA,","                       inherit.aes = TRUE) {","  ggplot2::layer(","    data = data,","    mapping = mapping,","    stat = StatEwcdf,","    geom = geom,","    position = position,","    show.legend = show.legend,","    inherit.aes = inherit.aes,","    params = list(","      n = n,","      pad = pad,","      na.rm = na.rm,","      ...","    )","  )","}","","","#' @title StatEwcdf ggproto object","#' @name StatEwcdf","#' @rdname StatEwcdf","#' @aliases StatEwcdf","#' @format NULL","#' @usage NULL","#' @export","#' @importFrom ggplot2 aes after_stat has_flipped_aes Stat","NULL","","StatEwcdf <- ggplot2::ggproto(","  \"StatEwcdf\", ggplot2::Stat,","  required_aes = c(\"x|y\", \"weights\"),","  dropped_aes = c(\"weights\"),     ","  ","  default_aes = ggplot2::aes(y = ggplot2::after_stat(y)),","  ","  setup_params = function(data, params) {","    params$flipped_aes <-","      ggplot2::has_flipped_aes(data,","                               params,","                               main_is_orthogonal = FALSE,","                               main_is_continuous = TRUE)","    ","    has_x <- !(is.null(data$x) && is.null(params$x))","    has_y <- !(is.null(data$y) && is.null(params$y))","    if (!has_x && !has_y) {","      rlang::abort(\"stat_ewcdf() requires an x or y aesthetic.\")","    }","    has_weights <- !(is.null(data$weights) && is.null(params$weights))","    #    if (!has_weights) {","    #      rlang::abort(\"stat_ewcdf() requires a weights aesthetic.\")","    #    }","    ","    params","  },","  ","  compute_group = function(data, scales, n = NULL, pad = TRUE, flipped_aes = FALSE) {","    data <- flip_data(data, flipped_aes)","    # If n is NULL, use raw values; otherwise interpolate","    if (is.null(n)) {","      x <- unique(data$x)","    } else {","      x <- seq(min(data$x), max(data$x), length.out = n)","    }","    ","    if (pad) {","      x <- c(-Inf, x, Inf)","    }","    if (is.null(data$weights)) {","      data_ecdf <- ecdf(data$x)(x)","    } else {","      data_ecdf <-","        spatstat.geom::ewcdf(","          data$x,","          weights = data$weights / sum(abs(data$weights)) ","        )(x)","    }","    ","    df_ecdf <- vctrs::new_data_frame(list(x = x, y = data_ecdf), n = length(x))","    df_ecdf$flipped_aes <- flipped_aes","    ggplot2::flip_data(df_ecdf, flipped_aes)","  }",")","","load(\"filament1.rda\")","library(styler)","library(ggplot2)","library(dplyr)","","#' neg_log_like","#'","#' compute the negative log-likelihood for a given set of parameters,","#' data, and model specification. ","#' ","#' @param beta These parameters are used to define the mean and standard deviation","#' of the normal distribution that models the relationship between the predictor(s) ","#' and the response variable.","#' @param data data frame containing the observed data, including CAD_Weight and Actual Weight","#' @param model A character string that specifies the model to be used,including \"A\" and \"B\",","","neg_log_like <- function(beta, data, model) {","  x <- data$CAD_Weight","  y <- data$Actual_Weight","  ","  if (model == \"A\") {","    mu <- beta[1] + beta[2] * x","    sigma <- exp(beta[3] + beta[4] * x)","  } else if (model == \"B\") {","    mu <- beta[1] + beta[2] * x","    sigma <- exp(beta[3]) + exp(beta[4]) * x^2","  }","  ","  neg_log_likelihood <- -sum(dnorm(y, mean = mu, sd = sqrt(sigma), log = TRUE))","  return(neg_log_likelihood)","}","","#' filament1_estimate","#' ","#'  estimate the parameters of two specified models (\"A\" or \"B\") ","#'  by minimizing the negative log-likelihood of the data given the model.","#' ","#' @param data containing the observed values, including","#' the variables used by the neg_log_like function, which are CAD_Weight","#' and Actual_Weight.","#' @param model A character string that specifies the model to be used,","#' including \"A\" and \"B\", each model with a different initial value for optimization.","","filament1_estimate <- function(data, model) {","  if (model == \"A\") {","    init_values <- c(-0.1, 1.07, -2, 0.05)","  } else if (model == \"B\") {","    init_values <- c(-0.15, 1.07, -13.5, -6.5)","  }","  ","  optim_res <- optim(par = init_values, fn = neg_log_like, data = data, model = model, hessian = TRUE, method = \"BFGS\")","  ","  return(list(estimate = optim_res$par, hessian = optim_res$hessian))","}","","#' compute_CI","#' ","#' It leverages the Hessian matrix obtained during optimization to ","#' compute the standard errors of the parameter estimates, which are then ","#' used to construct the 90% confidence intervals for model estimator returned","#' by filament1_estimate function","#' ","#' @param fit A list containing at least two components, which are estimate and ","#' hessian. Estimate is the vector of parameter estimates obtained from the optimization.","#' Hessian is the hessian matrix at the optimal parameters.","","compute_CI <- function(fit) {","  se <- sqrt(diag(solve(fit$hessian)))","  ","  # Z value for 90% confidence interval","  z_value <- qnorm(0.95) ","  ","  # Calculate the confidence intervals","  lower_bound <- fit$estimate - z_value * se","  upper_bound <- fit$estimate + z_value * se","  ","  return(data.frame(Estimate = fit$estimate, Lower = lower_bound, Upper = upper_bound, Std.Error = se))","}","","#' log_prior_density","#'","#' calculate the logarithm of the joint prior density of the parameters theta.","#' @param theta a numeric vector representing the parameters theta_1 to theta_4","#' @param params another numeric vector representing the gamma parameters from","#' gamma_1 to gamma_4","","log_prior_density <- function(theta, params) {","  # Extract the hyper parameters","  gamma1 <- params[1]","  gamma2 <- params[2]","  gamma3 <- params[3]","  gamma4 <- params[4]","  ","  # Calculate the log prior density for each parameter","  log_prior_theta1 <- dnorm(theta[1], mean = 0, sd = sqrt(gamma1), log = TRUE)","  log_prior_theta2 <- dnorm(theta[2], mean = 1, sd = sqrt(gamma2), log = TRUE)","  log_prior_theta3 <- dlogexp(exp(theta[3]), rate = gamma3, log = TRUE) ","  log_prior_theta4 <- dlogexp(exp(theta[4]), rate = gamma4, log = TRUE)","  ","  # Sum the log priors to get the joint log prior density","  log_joint_prior <- log_prior_theta1 + log_prior_theta2 + log_prior_theta3 + log_prior_theta4","  ","  return(log_joint_prior)","}","","#' log_like","#' ","#' evaluate the observation log-likelihood p(y|theta) for a specified model,","#' taking into account the parameters theta and data points x and y","#' @param theta a numeric vector representing the parameters theta_1 to theta_4","#' @param x the independent variable","#' @param y the response variable","","log_like <- function(theta, x, y) {","  beta1 <- theta[1]","  beta2 <- theta[2]","  beta3 <- exp(theta[3])","  beta4 <- exp(theta[4])","  ","  mu <- beta1 + beta2 * x","  sigma <- sqrt(beta3 + beta4 * x^2)","  ","  # Compute the log-likelihood for each observation","  log_lik_vector <- dnorm(y, mean = mu, sd = sigma, log = TRUE)","  ","  # Return the sum of the log-likelihoods","  sum_log_lik <- sum(log_lik_vector)","  return(sum_log_lik)","}","","#' log_posterior_density","#' ","#' evaluates the logarithm of the posterior density p(theta|y)","#' @param theta a numeric vector representing the parameters theta_1 to theta_4","#' @param x the independent variable","#' @param y the response variable","","log_posterior_density <- function(theta, x, y, params) {","  # Compute the log-likelihood","  log_lik <- log_like(theta, x, y)","  ","  # Compute the log-prior density","  log_prior <- log_prior_density(theta, params)","  ","  # The log-posterior is the sum of the log-likelihood and log-prior","  log_post <- log_lik + log_prior","  ","  return(log_post)","}","","","posterior_mode <- function(theta_start, x, y, params) {","  # Negated log-posterior density function to use with optim","  neg_log_posterior <- function(theta) -log_posterior_density(theta, x, y, params)","  ","  # Use optim to maximize the log-posterior density","  optim_result <- optim(theta_start, neg_log_posterior, method = \"BFGS\", hessian = TRUE)","  ","  # Compute the inverse of the negated Hessian","  S <- solve(-optim_result$hessian)","  ","  # Return a list with mode, Hessian, and S","  return(list(mode = optim_result$par, hessian = optim_result$hessian, S = S))","}","","do_importance <- function(N, mu, S, ...) {","  # Sample from the multivariate normal distribution","  samples <- mvtnorm::rmvnorm(N, mean = mu, sigma = S)","  ","  # Calculate the log-likelihoods of these samples using dmvnorm","  log_densities <- mvtnorm::dmvnorm(samples, mean = mu, sigma = S, log = TRUE)","  ","  # Calculate log_weights (importance weights)","  log_weights <- -log_densities ","  ","  # Normalize log_weights","  max_log_weight <- max(log_weights)","  log_weights <- log_weights - (max_log_weight + log(sum(exp(log_weights - max_log_weight))))","  ","  # Create the data.frame","  results <- data.frame(beta1 = samples[,1], beta2 = samples[,2], ","                        beta3 = samples[,3], beta4 = samples[,4], ","                        log_weights = log_weights)","  ","  return(results)","}","","","# Define a function to make credible intervals","make_CI <- function(x, weights, prob) {","  quantiles <- quantile(x, probs = c((1 - prob)/2, 1 - (1 - prob)/2), type = 1, na.rm = TRUE, weights = weights)","  data.frame(lower = quantiles[1], upper = quantiles[2])","}"],"eval":true,"echo":false,"results":"hide","label":"unnamed-chunk-1","dev":"png"},"document_id":"219634DD","chunk_id":"c3dbotq6q2ksf","chunk_label":"unnamed-chunk-1"},{"row":54,"row_count":1,"visible":true,"expansion_state":0,"options":{"engine":"r","eval":true,"echo":true,"label":"unnamed-chunk-2","dev":"png"},"document_id":"219634DD","chunk_id":"cfztnzpcuz5b2","chunk_label":"unnamed-chunk-2"},{"row":69,"row_count":1,"visible":true,"expansion_state":0,"options":{"engine":"r","eval":true,"echo":false,"label":"unnamed-chunk-3","dev":"png"},"document_id":"219634DD","chunk_id":"czce84wqp2987","chunk_label":"unnamed-chunk-3"},{"row":83,"row_count":1,"visible":true,"expansion_state":0,"options":{"engine":"r","eval":true,"echo":false,"label":"unnamed-chunk-4","dev":"png"},"document_id":"219634DD","chunk_id":"c5wyp7lko4rbz","chunk_label":"unnamed-chunk-4"},{"row":112,"row_count":1,"visible":true,"expansion_state":0,"options":{"engine":"r","eval":true,"echo":false,"label":"unnamed-chunk-5","dev":"png"},"document_id":"219634DD","chunk_id":"c9xeyo1s1gcw4","chunk_label":"unnamed-chunk-5"},{"row":192,"row_count":1,"visible":true,"expansion_state":0,"options":{"engine":"r","label":"unnamed-chunk-30","dev":"png"},"document_id":"219634DD","chunk_id":"csb2vfhl0dshh","chunk_label":"unnamed-chunk-7"},{"row":237,"row_count":1,"visible":true,"expansion_state":0,"options":{"engine":"r","label":"unnamed-chunk-23","dev":"png"},"document_id":"219634DD","chunk_id":"cghfe596t4zx8","chunk_label":"unnamed-chunk-9"},{"row":264,"row_count":1,"visible":true,"expansion_state":0,"options":{"engine":"r","echo":false,"warning":false,"label":"unnamed-chunk-28","dev":"png"},"document_id":"219634DD","chunk_id":"c89kgkelem55n","chunk_label":"unnamed-chunk-10"},{"row":280,"row_count":1,"visible":true,"expansion_state":0,"options":{"engine":"r","echo":false,"label":"unnamed-chunk-29","dev":"png"},"document_id":"219634DD","chunk_id":"c1y2aiis48vyw","chunk_label":"unnamed-chunk-11"},{"row":183,"row_count":1,"visible":true,"expansion_state":0,"options":{"engine":"r","eval":true,"echo":true,"label":"unnamed-chunk-26","dev":"png"},"document_id":"219634DD","chunk_id":"cibalw7moijw1","chunk_label":"unnamed-chunk-6"},{"row":226,"row_count":1,"visible":true,"expansion_state":0,"options":{"engine":"r","echo":false,"label":"unnamed-chunk-31","dev":"png"},"document_id":"219634DD","chunk_id":"cpbe7ir3ou24a","chunk_label":"unnamed-chunk-8"}],"doc_write_time":1708929131,"working_dir":null,"default_chunk_options":{"echo":true,"eval":true}}